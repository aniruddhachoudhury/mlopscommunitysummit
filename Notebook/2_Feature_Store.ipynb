{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store with Kubeflow Orchestration\n",
    "## Contents\n",
    "- <a href='#1'>1. Load Python libraries and importing the data</a>  \n",
    "\n",
    "\n",
    "- <a href='#2'>2. Feature Store Client initialization</a> \n",
    "    - <a href='#2.1'>2.1. Declare Enviorment Variables for feast</a> \n",
    "    - <a href='#2.2'>2.2. Create GCS Bucket</a> \n",
    "    - <a href='#2.3'>2.3. Feature Store Client: Used for creating, managing, and retrieving features</a> \n",
    "    - <a href='#2.4'>2.4. Load Data</a> \n",
    "    \n",
    "    \n",
    "- <a href='#3'>3. Feature Registry</a>\n",
    "    - <a href='#3.1'>3.1. Declare Features and Entities</a> \n",
    "    - <a href='#3.2'>3.2. Declare  Feature Table Schema</a> \n",
    "    - <a href='#3.2'>3.3. Registering entities and feature tables in Feature Store</a> \n",
    "    - <a href='#3.2'>3.4. Populating batch source Ingestion</a> \n",
    "    - <a href='#3.3'>3.5. Check the Feature Table stats</a>\n",
    "    \n",
    "    \n",
    "    \n",
    "- <a href='#4'>4. Batch Source Online Ingestion</a>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- <a href='#5'>5. Online Storage with Batch Ingestion</a>\n",
    "    - <a href='#5.1'>5.1. Util Kafka Function</a> \n",
    "    - <a href='#5.2'>5.2. List Kafka Intializer</a> \n",
    "    - <a href='#5.3'>5.3. Trip Data Kakfa Ingestion</a> \n",
    "    - <a href='#5.4'>5.4. Fare Data kafka Ingestion</a> \n",
    "    \n",
    "\n",
    "- <a href='#6'>6. Kubeflow Pipeline Artifacts for Model Training </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Python libraries and importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from feast.config import Config\n",
    "from feast.data_source import FileSource, KafkaSource\n",
    "from feast.data_format import ParquetFormat, AvroFormat\n",
    "from feast import Client, Feature, Entity, ValueType, FeatureTable\n",
    "from feast.pyspark.abc import RetrievalJobParameters, SparkJobStatus, SparkJob\n",
    "import feast.staging.entities as entities\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install feast==0.9\n",
    "!pip install protobuf gcsfs  -U -q\n",
    "!pip install --upgrade pip\n",
    "!pip install google-cloud-dataproc==2.3.1\n",
    "!pip install confluent_kafka\n",
    "!pip install kafka-python\n",
    "!pip install avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 Feature Store Client initialization\n",
    "\n",
    "Feast contains the following core concepts:\n",
    "\n",
    "* **Projects:** Serve as a top level namespace for all Feast resources. Each project is a completely independent environment in Feast. Users can only work in a single project at a time.\n",
    "* **Entities:** Entities are the objects in an organization on which features occur. They map to your business domain \\(users, products, transactions, locations\\).\n",
    "* **Feature Tables:** Defines a group of features that occur on a specific entity.\n",
    "* **Features:** Individual feature within a feature table.\n",
    "\n",
    "### 2.1 Declare Enviorment Variables for feast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Run the following Command by connecting to Kubernetes Cluster\n",
    "\n",
    "%%bash\n",
    "```\n",
    "kubectl get svc \n",
    "\n",
    "```\n",
    "- Copy the Redis & Kafka IP and paste in below variables \n",
    "- Copy the Project ID and paste below\n",
    "- Copy the Dataproc Cluster Name and GCS Staging bucket\n",
    "- Copy the GCS feast staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class feature_store_client:\n",
    "    \n",
    "    def __init__(self,env,bucket):\n",
    "        \n",
    "        self.env=env\n",
    "        self.staging_bucket=bucket\n",
    "        \n",
    "    def feature_store_settings(self):\n",
    "        \n",
    "        if self.env.lower()==\"dataproc\":\n",
    "            # Using environmental variables\n",
    "            environment = {'FEAST_CORE_URL': 'feast-release-feast-core.default:6565',\n",
    "                         'FEAST_DATAPROC_CLUSTER_NAME': 'dataprocfeast',\n",
    "                         'FEAST_DATAPROC_PROJECT': '<BUCKET>',\n",
    "                         'FEAST_DATAPROC_REGION': 'us-east1',\n",
    "                         'FEAST_STAGING_LOCATION': self.staging_bucket,\n",
    "                         'FEAST_HISTORICAL_FEATURE_OUTPUT_FORMAT': 'parquet',\n",
    "                         'FEAST_HISTORICAL_FEATURE_OUTPUT_LOCATION': f\"{self.staging_bucket}historical\" ,\n",
    "                         'FEAST_HISTORICAL_SERVING_URL': 'feast-release-feast-online-serving.default:6566',\n",
    "                         'FEAST_REDIS_HOST': '<REDIS_IP>',\n",
    "                         'FEAST_REDIS_PORT': '6379',\n",
    "                         'FEAST_SERVING_URL': 'feast-release-feast-online-serving.default:6566',\n",
    "                         'FEAST_SPARK_HOME': '/usr/local/spark',\n",
    "                         'FEAST_SPARK_LAUNCHER': 'dataproc',\n",
    "                         'FEAST_SPARK_STAGING_LOCATION': 'gs://dataproc-staging-us-east1-996861042416-4w01soni/artifacts/',\n",
    "                         'FEAST_SPARK_STANDALONE_MASTER': 'local[*]',\n",
    "                         'STAGING_BUCKET': 'self.staging_bucket',\n",
    "                         'DEMO_KAFKA_BROKERS': '<KAFKA_IP>'\n",
    "                           \n",
    "                          }              \n",
    "     \n",
    "            for key,value in environment.items():\n",
    "                os.environ[key] = value \n",
    "            \n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_staging_bucket():\n",
    "    staging_bucket = f'gs://feast-staging-bucket-{random.randint(1000000, 10000000)}/'\n",
    "    !gsutil mb {staging_bucket}\n",
    "    print(f'Staging bucket is {staging_bucket}')\n",
    "    return staging_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Bucket\n",
    "staging_bucket=create_staging_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env=feature_store_client('Dataproc',staging_bucket)\n",
    "set_env.feature_store_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Store Client: Used for creating, managing, and retrieving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FEAST_CORE_URL': 'feast-release-feast-core.default:6565',\n",
      " 'FEAST_DATAPROC_CLUSTER_NAME': 'dataprocfeast',\n",
      " 'FEAST_DATAPROC_PROJECT': '<BUCKET>',\n",
      " 'FEAST_DATAPROC_REGION': 'us-east1',\n",
      " 'FEAST_HISTORICAL_FEATURE_OUTPUT_FORMAT': 'parquet',\n",
      " 'FEAST_HISTORICAL_FEATURE_OUTPUT_LOCATION': 'gs://feast-staging-bucket-9919526/historical',\n",
      " 'FEAST_HISTORICAL_SERVING_URL': 'feast-release-feast-online-serving.default:6566',\n",
      " 'FEAST_REDIS_HOST': '<REDIS_IP>',\n",
      " 'FEAST_REDIS_PORT': '6379',\n",
      " 'FEAST_SERVING_URL': 'feast-release-feast-online-serving.default:6566',\n",
      " 'FEAST_SPARK_HOME': '/usr/local/spark',\n",
      " 'FEAST_SPARK_LAUNCHER': 'dataproc',\n",
      " 'FEAST_SPARK_STAGING_LOCATION': 'gs://dataproc-staging-us-east1-996861042416-4w01soni/artifacts/',\n",
      " 'FEAST_SPARK_STANDALONE_MASTER': 'local[*]',\n",
      " 'FEAST_STAGING_LOCATION': 'gs://feast-staging-bucket-9919526/'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "pprint({key: value for key, value in os.environ.items() if key.startswith(\"FEAST_\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "client = Client()\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>target</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610685</td>\n",
       "      <td>2009-06-15 17:26:21+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.354113</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825735</td>\n",
       "      <td>2010-01-05 16:52:16+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.088648</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>428317</td>\n",
       "      <td>2011-08-18 00:35:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.813646</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356886</td>\n",
       "      <td>2012-04-21 04:30:42+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.191734</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>603801</td>\n",
       "      <td>2010-03-09 07:51:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.975267</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id            pickup_datetime  passenger_count  fare_amount  target  \\\n",
       "0     610685  2009-06-15 17:26:21+00:00                1    -1.354113       0   \n",
       "1     825735  2010-01-05 16:52:16+00:00                1     1.088648       1   \n",
       "2     428317  2011-08-18 00:35:00+00:00                2    -0.813646       0   \n",
       "3     356886  2012-04-21 04:30:42+00:00                1    -0.191734       0   \n",
       "4     603801  2010-03-09 07:51:00+00:00                1    -0.975267       0   \n",
       "\n",
       "                      created  \n",
       "0  2021-06-12 16:09:45.131085  \n",
       "1  2021-06-12 16:09:45.131085  \n",
       "2  2021-06-12 16:09:45.131085  \n",
       "3  2021-06-12 16:09:45.131085  \n",
       "4  2021-06-12 16:09:45.131085  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_details=pd.read_csv(\"faredetails.csv\")\n",
    "fare_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>key</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>longitude_distance</th>\n",
       "      <th>latitude_distance</th>\n",
       "      <th>distance_travelled</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610685</td>\n",
       "      <td>2009-06-15 17:26:21.000000100</td>\n",
       "      <td>2009-06-15 17:26:21+00:00</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825735</td>\n",
       "      <td>2010-01-05 16:52:16.000000200</td>\n",
       "      <td>2010-01-05 16:52:16+00:00</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.079696</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>428317</td>\n",
       "      <td>2011-08-18 00:35:00.000000490</td>\n",
       "      <td>2011-08-18 00:35:00+00:00</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356886</td>\n",
       "      <td>2012-04-21 04:30:42.000000100</td>\n",
       "      <td>2012-04-21 04:30:42+00:00</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>603801</td>\n",
       "      <td>2010-03-09 07:51:00.000000135</td>\n",
       "      <td>2010-03-09 07:51:00+00:00</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id                            key            pickup_datetime  \\\n",
       "0     610685  2009-06-15 17:26:21.000000100  2009-06-15 17:26:21+00:00   \n",
       "1     825735  2010-01-05 16:52:16.000000200  2010-01-05 16:52:16+00:00   \n",
       "2     428317  2011-08-18 00:35:00.000000490  2011-08-18 00:35:00+00:00   \n",
       "3     356886  2012-04-21 04:30:42.000000100  2012-04-21 04:30:42+00:00   \n",
       "4     603801  2010-03-09 07:51:00.000000135  2010-03-09 07:51:00+00:00   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.844311        40.721319         -73.841610         40.712278   \n",
       "1        -74.016048        40.711303         -73.979268         40.782004   \n",
       "2        -73.982738        40.761270         -73.991242         40.750562   \n",
       "3        -73.987130        40.733143         -73.991567         40.758092   \n",
       "4        -73.968095        40.768008         -73.956655         40.783762   \n",
       "\n",
       "   longitude_distance  latitude_distance  distance_travelled  \\\n",
       "0            0.002701           0.009041            0.009436   \n",
       "1            0.036780           0.070701            0.079696   \n",
       "2            0.008504           0.010708            0.013674   \n",
       "3            0.004437           0.024949            0.025340   \n",
       "4            0.011440           0.015754            0.019470   \n",
       "\n",
       "                      created  \n",
       "0  2021-06-12 16:09:45.131085  \n",
       "1  2021-06-12 16:09:45.131085  \n",
       "2  2021-06-12 16:09:45.131085  \n",
       "3  2021-06-12 16:09:45.131085  \n",
       "4  2021-06-12 16:09:45.131085  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_details=pd.read_csv(\"tripdetails.csv\")\n",
    "trip_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_datetime(df,col):\n",
    "    df[col]=pd.to_datetime(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_details=change_datetime(fare_details,'pickup_datetime')\n",
    "fare_details=change_datetime(fare_details,'created')\n",
    "trip_details=change_datetime(trip_details,'created')\n",
    "trip_details=change_datetime(trip_details,'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_details.drop(columns='key',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(driver_id                           int64\n",
       " pickup_datetime       datetime64[ns, UTC]\n",
       " pickup_longitude                  float64\n",
       " pickup_latitude                   float64\n",
       " dropoff_longitude                 float64\n",
       " dropoff_latitude                  float64\n",
       " longitude_distance                float64\n",
       " latitude_distance                 float64\n",
       " distance_travelled                float64\n",
       " created                    datetime64[ns]\n",
       " dtype: object, driver_id                        int64\n",
       " pickup_datetime    datetime64[ns, UTC]\n",
       " passenger_count                  int64\n",
       " fare_amount                    float64\n",
       " target                           int64\n",
       " created                 datetime64[ns]\n",
       " dtype: object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_details.dtypes,fare_details.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Features Registry\n",
    "### 3.1 Declare Features and Entities\n",
    "Entity defines the primary key(s) associated with one or more feature tables. The entity must be registered before declaring the associated feature tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class features_entities:\n",
    "    \n",
    "    def __init__(self,df,entity_col,columns_list,name):\n",
    "        \n",
    "        self.df=df\n",
    "        self.entity_col=entity_col\n",
    "        self.columns_list=columns_list\n",
    "        self.feature_register=name\n",
    "        #self.tags=labels\n",
    "        \n",
    "        \n",
    "    def entity(self):\n",
    "        if self.df[ self.entity_col].dtype == 'int64':\n",
    "            driver_id = Entity(name=self.entity_col, description=\"Driver identifier\", value_type=ValueType.INT64,labels={\"release\": \"prod-env\", \"description\": f\"This prod feature {self.entity_col} means the unique entity of driver\",\"production\": \"model\"})\n",
    "            return driver_id\n",
    "    \n",
    "    \n",
    "    def trip_labelss(trip_columns,i):\n",
    "        labels={\n",
    "           \"tripdata\":[\n",
    "            {\"release\": \"prod-env\", \"description\": f\"This prod feature {trip_columns[i]} means the longitude coordinate of where the taxi ride started\",\"production\": \"model\"},\n",
    "             {\"release\": \"dev-env\", \"description\": f\"This dev feature {trip_columns[i]} means the latitude coordinate of where the taxi ride started\",\"production\": \"model\"},\n",
    "            {\"release\": \"prod-env\", \"description\": f\"This prod feature {trip_columns[i]} means longitude coordinate of where the taxi ride ended\",\"production\": \"train\"},\n",
    "            {\"release\": \"prod-env\", \"description\": f\"This prod feature {trip_columns[i]} means latitude coordinate of where the taxi ride ended.\",\"production\": \"model\"},\n",
    "             {\"release\": \"dev-env\", \"description\": f\"This dev feature {trip_columns[i]} means the longitude  distance of the trips in City\",\"production\": \"train\"},\n",
    "             {\"release\": \"dev-env\", \"description\": f\"This dev feature {trip_columns[i]} means the latitude  distance of the trips in City\",\"production\": \"train\"},\n",
    "            {\"release\": \"prod-env\", \"description\": f\"This prod feature {trip_columns[i]} means the total distance covered by a trip\",\"production\": \"test\"},\n",
    "           ]}\n",
    "    \n",
    "        return labels['tripdata'][i]\n",
    "    \n",
    "    \n",
    "    def fare_labelss(trip_columns,i):\n",
    "        labels={\n",
    "           \"faredata\":[\n",
    "            {\"release\": \"prod-env\", \"description\": f\"This prod feature {trip_columns[i]} means the total passenger count\",\"production\": \"test\"},\n",
    "             {\"release\": \"dev-env\", \"description\": f\"This dev feature {trip_columns[i]} means the total fare amount for the trip\",\"production\": \"entity\"},\n",
    "            {\"release\": \"dev-env\", \"description\": f\"This prod feature {trip_columns[i]} means the target profit and loss for the trip\",\"production\": \"target\"},\n",
    "           \n",
    "           ]}\n",
    "    \n",
    "        return labels['faredata'][i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def trip_features(self):\n",
    "       \n",
    "            features=[]\n",
    "            retrieve_features=[]\n",
    "            count=0\n",
    "            for i in  range(len(self.columns_list)):\n",
    "            \n",
    "                if  self.df.dtypes[self.columns_list[i]] == 'int64':\n",
    "                    #print(trip_labelss(self.columns_list,i))\n",
    "                    local_features=Feature(f\"{self.columns_list[i]}\", ValueType.INT64,labels=features_entities.trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'float64':\n",
    "                    #print(trip_labelss(self.columns_list,i))\n",
    "                    local_features=Feature(f\"{self.columns_list[i]}\", ValueType.DOUBLE,labels=features_entities.trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i],)\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'int32':\n",
    "                    #print(self.tags['tripdata'][count])\n",
    "                    local_features=Feature(f\"{i}\", ValueType.INT32,labels=trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'object':\n",
    "                    #print(self.tags['tripdata'][count])\n",
    "                    local_features=Feature(f\"{i}\", ValueType.STRING,labels=trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'bytes':\n",
    "                    local_features=Feature(f\"{i}\", ValueType.BYTES,labels=trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'bool':\n",
    "                    local_features=Feature(f\"{i}\", ValueType.BOOL,labels=trip_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "                count+=1\n",
    "            return features,retrieve_features\n",
    "    \n",
    "    \n",
    "    def fare_features(self):\n",
    "       \n",
    "            features=[]\n",
    "            retrieve_features=[]\n",
    "            count=0\n",
    "            for i in  range(len(self.columns_list)):\n",
    "     \n",
    "                if  self.df.dtypes[self.columns_list[i]] == 'int64':\n",
    "                    #print(trip_labelss(self.columns_list,i))\n",
    "                    local_features=Feature(f\"{self.columns_list[i]}\", ValueType.INT64,labels=features_entities.fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'float64':\n",
    "                    #print(trip_labelss(self.columns_list,i))\n",
    "                    local_features=Feature(f\"{self.columns_list[i]}\", ValueType.DOUBLE,labels=features_entities.fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i],)\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'int32':\n",
    "                    #print(self.tags['tripdata'][count])\n",
    "                    local_features=Feature(f\"{i}\", ValueType.INT32,labels=fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'object':\n",
    "                    #print(self.tags['tripdata'][count])\n",
    "                    local_features=Feature(f\"{i}\", ValueType.STRING,labels=fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'bytes':\n",
    "                    local_features=Feature(f\"{i}\", ValueType.BYTES,labels=fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "\n",
    "                elif  self.df.dtypes[self.columns_list[i]] == 'bool':\n",
    "                    local_features=Feature(f\"{i}\", ValueType.BOOL,labels=fare_labelss(self.columns_list,i))\n",
    "                    features.append(local_features)\n",
    "                    retrieve_features.append(self.feature_register+':'+self.columns_list[i])\n",
    "                count+=1\n",
    "            return features,retrieve_features\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "           \n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Completed the Feature and Entity Declaration'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  ### Trip Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'longitude_distance',\n",
       " 'latitude_distance',\n",
       " 'distance_travelled']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_columns=list(trip_details.columns[2:-1])\n",
    "trip_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_declaration=features_entities(trip_details,'driver_id',trip_columns,'trip_statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entity is any domain object that can be modeled and about which information can be stored. Entities are usually recognizable concepts, either concrete or abstract, such as persons, places, things, or events.\n",
    "\n",
    "Examples of entities in the context of ride-hailing and food delivery: `customer`, `order`, `driver`, `restaurant`, `dish`, `area`.\n",
    "\n",
    "Feast uses entities in the following way:\n",
    "\n",
    "* Entities serve as the keys used to look up features for producing training datasets and online feature values.\n",
    "* Entities serve as a natural grouping of features in a feature table. A feature table must belong to an entity \\(which could be a composite entity\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_entity=feature_declaration.entity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_entity,driver_features=feature_declaration.trip_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_statistics:pickup_longitude',\n",
       " 'trip_statistics:pickup_latitude',\n",
       " 'trip_statistics:dropoff_longitude',\n",
       " 'trip_statistics:dropoff_latitude',\n",
       " 'trip_statistics:longitude_distance',\n",
       " 'trip_statistics:latitude_distance',\n",
       " 'trip_statistics:distance_travelled']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  ### Fare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passenger_count', 'fare_amount', 'target']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_columns=list(fare_details.columns[2:-1])\n",
    "fare_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_declaration=features_entities(fare_details,'driver_id',fare_columns,'fare_statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the Feature and Entity Declaration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<feast.feature.Feature at 0x7fb8580e9d30>,\n",
       " <feast.feature.Feature at 0x7fb8580e9f28>,\n",
       " <feast.feature.Feature at 0x7fb8580e9f98>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_entity,fare_features=feature_declaration.fare_features()\n",
    "print(feature_declaration)\n",
    "fare_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fare_statistics:passenger_count',\n",
       " 'fare_statistics:fare_amount',\n",
       " 'fare_statistics:target']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Declare  Feature Table Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature tables are both a schema and a logical means of grouping features, data sources, and other related metadata.\n",
    "\n",
    "Feature tables serve the following purposes:\n",
    "\n",
    "- Feature tables are a means for defining the location and properties of data sources.\n",
    "- Feature tables are used to create within Feast a database-level structure for the storage of feature values.\n",
    "- The data sources described within feature tables allow Feast to find and ingest feature data into stores within Feast.\n",
    "- Feature tables ensure data is efficiently stored during ingestion by providing a grouping mechanism of features values that occur on the same event timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://feast-staging-bucket-9919526/test_data'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the location we're using for the offline feature store.\n",
    "demo_data_location = os.path.join(os.getenv('FEAST_STAGING_LOCATION'), \"test_data\")\n",
    "demo_data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_source_uri = os.path.join(demo_data_location, \"driver_statistics\")\n",
    "\n",
    "trip_statistics = FeatureTable(\n",
    "    name = \"trip_statistics\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = trip_entity,\n",
    "    batch_source=FileSource(\n",
    "        event_timestamp_column=\"pickup_datetime\",\n",
    "        created_timestamp_column=\"created\",\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=trip_source_uri,\n",
    "        date_partition_column=\"date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_source_uri = os.path.join(demo_data_location, \"fare_statistics\")\n",
    "\n",
    "\n",
    "fare_statistics = FeatureTable(\n",
    "    name = \"fare_statistics\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = fare_entity,\n",
    "    batch_source=FileSource(\n",
    "        event_timestamp_column=\"pickup_datetime\",\n",
    "        created_timestamp_column=\"created\",\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=fare_source_uri,\n",
    "        date_partition_column=\"date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Registering entities and feature tables in Feature Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class register_tables:\n",
    "    \n",
    "    def __init__(self,client):\n",
    "        self.feast_client=client\n",
    "    \n",
    "    def register(self):\n",
    "        self.feast_client.apply(driver_entity)\n",
    "        self.feast_client.apply(trip_statistics)\n",
    "        self.feast_client.apply(fare_statistics)\n",
    "    \n",
    "    def __str__(self):\n",
    "        for i in ['trip_statistics','fare_statistics']:\n",
    "            print(self.feast_client.get_feature_table(i).to_yaml())\n",
    "        return \"Done Registration\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_register=register_tables(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_register.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec:\n",
      "  name: trip_statistics\n",
      "  entities:\n",
      "  - driver_id\n",
      "  features:\n",
      "  - name: latitude_distance\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      release: dev-env\n",
      "      description: This dev feature latitude_distance means the latitude  distance\n",
      "        of the trips in City\n",
      "      production: train\n",
      "  - name: longitude_distance\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      production: train\n",
      "      description: This dev feature longitude_distance means the longitude  distance\n",
      "        of the trips in City\n",
      "      release: dev-env\n",
      "  - name: dropoff_latitude\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      description: This prod feature dropoff_latitude means latitude coordinate of\n",
      "        where the taxi ride ended.\n",
      "      production: model\n",
      "      release: prod-env\n",
      "  - name: pickup_longitude\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      release: prod-env\n",
      "      production: model\n",
      "      description: This prod feature pickup_longitude means the longitude coordinate\n",
      "        of where the taxi ride started\n",
      "  - name: dropoff_longitude\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      description: This prod feature dropoff_longitude means longitude coordinate\n",
      "        of where the taxi ride ended\n",
      "      production: train\n",
      "      release: prod-env\n",
      "  - name: pickup_latitude\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      description: This dev feature pickup_latitude means the latitude coordinate\n",
      "        of where the taxi ride started\n",
      "      release: dev-env\n",
      "      production: model\n",
      "  - name: distance_travelled\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      release: prod-env\n",
      "      description: This prod feature distance_travelled means the total distance covered\n",
      "        by a trip\n",
      "      production: test\n",
      "  batchSource:\n",
      "    type: BATCH_FILE\n",
      "    eventTimestampColumn: pickup_datetime\n",
      "    datePartitionColumn: date\n",
      "    createdTimestampColumn: created\n",
      "    fileOptions:\n",
      "      fileFormat:\n",
      "        parquetFormat: {}\n",
      "      fileUrl: gs://feast-staging-bucket-9919526/test_data/driver_statistics\n",
      "meta:\n",
      "  createdTimestamp: '2021-06-12T16:48:12Z'\n",
      "\n",
      "spec:\n",
      "  name: fare_statistics\n",
      "  entities:\n",
      "  - driver_id\n",
      "  features:\n",
      "  - name: passenger_count\n",
      "    valueType: INT64\n",
      "    labels:\n",
      "      release: prod-env\n",
      "      production: test\n",
      "      description: This prod feature passenger_count means the total passenger count\n",
      "  - name: target\n",
      "    valueType: INT64\n",
      "    labels:\n",
      "      description: This prod feature target means the target profit and loss for the\n",
      "        trip\n",
      "      release: dev-env\n",
      "      production: target\n",
      "  - name: fare_amount\n",
      "    valueType: DOUBLE\n",
      "    labels:\n",
      "      production: entity\n",
      "      description: This dev feature fare_amount means the total fare amount for the\n",
      "        trip\n",
      "      release: dev-env\n",
      "  batchSource:\n",
      "    type: BATCH_FILE\n",
      "    eventTimestampColumn: pickup_datetime\n",
      "    datePartitionColumn: date\n",
      "    createdTimestampColumn: created\n",
      "    fileOptions:\n",
      "      fileFormat:\n",
      "        parquetFormat: {}\n",
      "      fileUrl: gs://feast-staging-bucket-9919526/test_data/fare_statistics\n",
      "meta:\n",
      "  createdTimestamp: '2021-06-12T16:48:12Z'\n",
      "\n",
      "Done Registration\n"
     ]
    }
   ],
   "source": [
    "print(table_register)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Populating batch source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>longitude_distance</th>\n",
       "      <th>latitude_distance</th>\n",
       "      <th>distance_travelled</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610685</td>\n",
       "      <td>2009-06-15 17:26:21+00:00</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825735</td>\n",
       "      <td>2010-01-05 16:52:16+00:00</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.079696</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0     610685 2009-06-15 17:26:21+00:00        -73.844311        40.721319   \n",
       "1     825735 2010-01-05 16:52:16+00:00        -74.016048        40.711303   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  longitude_distance  latitude_distance  \\\n",
       "0         -73.841610         40.712278            0.002701           0.009041   \n",
       "1         -73.979268         40.782004            0.036780           0.070701   \n",
       "\n",
       "   distance_travelled                    created  \n",
       "0            0.009436 2021-06-12 16:09:45.131085  \n",
       "1            0.079696 2021-06-12 16:09:45.131085  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_details.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trip_details=trip_details.iloc[:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>target</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610685</td>\n",
       "      <td>2009-06-15 17:26:21+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.354113</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825735</td>\n",
       "      <td>2010-01-05 16:52:16+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.088648</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-06-12 16:09:45.131085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id           pickup_datetime  passenger_count  fare_amount  target  \\\n",
       "0     610685 2009-06-15 17:26:21+00:00                1    -1.354113       0   \n",
       "1     825735 2010-01-05 16:52:16+00:00                1     1.088648       1   \n",
       "\n",
       "                     created  \n",
       "0 2021-06-12 16:09:45.131085  \n",
       "1 2021-06-12 16:09:45.131085  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_details.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fare_details=fare_details.iloc[:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temporary file(s)...\n",
      "Data has been successfully ingested into FeatureTable batch source.\n"
     ]
    }
   ],
   "source": [
    "client.ingest(trip_statistics, trip_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temporary file(s)...\n",
      "Data has been successfully ingested into FeatureTable batch source.\n"
     ]
    }
   ],
   "source": [
    "client.ingest(fare_statistics, fare_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Check the Feature Table stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spec': {'name': 'trip_statistics',\n",
       "  'entities': ['driver_id'],\n",
       "  'features': [{'name': 'latitude_distance',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'release': 'dev-env',\n",
       "     'description': 'This dev feature latitude_distance means the latitude  distance of the trips in City',\n",
       "     'production': 'train'}},\n",
       "   {'name': 'longitude_distance',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'release': 'dev-env',\n",
       "     'production': 'train',\n",
       "     'description': 'This dev feature longitude_distance means the longitude  distance of the trips in City'}},\n",
       "   {'name': 'dropoff_latitude',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'production': 'model',\n",
       "     'release': 'prod-env',\n",
       "     'description': 'This prod feature dropoff_latitude means latitude coordinate of where the taxi ride ended.'}},\n",
       "   {'name': 'pickup_longitude',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'description': 'This prod feature pickup_longitude means the longitude coordinate of where the taxi ride started',\n",
       "     'release': 'prod-env',\n",
       "     'production': 'model'}},\n",
       "   {'name': 'dropoff_longitude',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'description': 'This prod feature dropoff_longitude means longitude coordinate of where the taxi ride ended',\n",
       "     'release': 'prod-env',\n",
       "     'production': 'train'}},\n",
       "   {'name': 'pickup_latitude',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'description': 'This dev feature pickup_latitude means the latitude coordinate of where the taxi ride started',\n",
       "     'release': 'dev-env',\n",
       "     'production': 'model'}},\n",
       "   {'name': 'distance_travelled',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'production': 'test',\n",
       "     'description': 'This prod feature distance_travelled means the total distance covered by a trip',\n",
       "     'release': 'prod-env'}}],\n",
       "  'batchSource': {'type': 'BATCH_FILE',\n",
       "   'eventTimestampColumn': 'pickup_datetime',\n",
       "   'datePartitionColumn': 'date',\n",
       "   'createdTimestampColumn': 'created',\n",
       "   'fileOptions': {'fileFormat': {'parquetFormat': {}},\n",
       "    'fileUrl': 'gs://feast-staging-bucket-9919526/test_data/driver_statistics'}}},\n",
       " 'meta': {'createdTimestamp': '2021-06-12T16:48:12Z'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_feature_table(name='trip_statistics').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spec': {'name': 'fare_statistics',\n",
       "  'entities': ['driver_id'],\n",
       "  'features': [{'name': 'passenger_count',\n",
       "    'valueType': 'INT64',\n",
       "    'labels': {'production': 'test',\n",
       "     'description': 'This prod feature passenger_count means the total passenger count',\n",
       "     'release': 'prod-env'}},\n",
       "   {'name': 'target',\n",
       "    'valueType': 'INT64',\n",
       "    'labels': {'production': 'target',\n",
       "     'release': 'dev-env',\n",
       "     'description': 'This prod feature target means the target profit and loss for the trip'}},\n",
       "   {'name': 'fare_amount',\n",
       "    'valueType': 'DOUBLE',\n",
       "    'labels': {'description': 'This dev feature fare_amount means the total fare amount for the trip',\n",
       "     'release': 'dev-env',\n",
       "     'production': 'entity'}}],\n",
       "  'batchSource': {'type': 'BATCH_FILE',\n",
       "   'eventTimestampColumn': 'pickup_datetime',\n",
       "   'datePartitionColumn': 'date',\n",
       "   'createdTimestampColumn': 'created',\n",
       "   'fileOptions': {'fileFormat': {'parquetFormat': {}},\n",
       "    'fileUrl': 'gs://feast-staging-bucket-9919526/test_data/fare_statistics'}}},\n",
       " 'meta': {'createdTimestamp': '2021-06-12T16:48:12Z'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_feature_table(name='fare_statistics').to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Batch Source Online Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to populate the online storage, we can use Feast SDK to start a Spark batch job which will extract the features from the batch source, then load the features to an online store.\n",
    "\n",
    "The online store maintains only the latest values for a specific feature.\n",
    "\n",
    "- Feature values are stored based on their entity keys\n",
    "- Feast currently supports Redis as an online store.\n",
    "- Online stores are meant for very high throughput writes from ingestion jobs and very low latency access to features during online serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.start_offline_to_online_ingestion(\n",
    "    fare_statistics,\n",
    "    datetime(2009, 1, 18),\n",
    "    datetime(2011, 10, 20)\n",
    ")\n",
    "job = client.start_offline_to_online_ingestion(\n",
    "    trip_statistics,\n",
    "    datetime(2009, 1, 18),\n",
    "    datetime(2011, 10, 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 Real Time Streaming Ingestion with Kakfa\n",
    "With a streaming source, we can use Feast SDK to launch a Spark streaming job that continuously update the online store. First, we will update `driver_trips` feature table such that a new streaming source is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pytz\n",
    "import io\n",
    "import kafka\n",
    "import avro.schema\n",
    "from avro.io import BinaryEncoder, DatumWriter\n",
    "from confluent_kafka import Producer\n",
    "from kafka import KafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from kafka import KafkaClient,KafkaAdminClient\n",
    "from kafka import KafkaConsumer\n",
    "import avro.schema\n",
    "import avro.io\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Util Kafka Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kafkatopic:\n",
    "    \n",
    "    def __init__(self,kafka_broker,topic):\n",
    "        self.KAFKA_BROKER=kafka_broker\n",
    "        self.topic=topic\n",
    "    \n",
    "    def create_topic(self):\n",
    "        a = AdminClient({'bootstrap.servers':  self.KAFKA_BROKER})\n",
    "\n",
    "        new_topics = [NewTopic(self.topic, num_partitions=3, replication_factor=1)]\n",
    "        # Note: In a multi-cluster production scenario, it is more typical to use a replication_factor of 3 for durability.\n",
    "\n",
    "        # Call create_topics to asynchronously create topics. A dict\n",
    "        # of <topic,future> is returned.\n",
    "        fs = a.create_topics(new_topics)\n",
    "\n",
    "        # Wait for each operation to finish.\n",
    "        for topic, f in fs.items():\n",
    "            try:\n",
    "                f.result()  # The result itself is None\n",
    "                print(\"Topic {} created\".format(topic))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to create topic {}: {}\".format(topic, e))\n",
    "   \n",
    "    def list_kafka_topic(self):\n",
    "        consumer = kafka.KafkaConsumer(bootstrap_servers=[self.KAFKA_BROKER])\n",
    "        return consumer.topics()\n",
    "    \n",
    "   \n",
    "    def kafka_consumer(self,avro_schema_json):\n",
    "        # To consume messages\n",
    "        consumer = KafkaConsumer(bootstrap_servers=[KAFKA_BROKER],auto_offset_reset='earliest')\n",
    "        consumer.subscribe([self.topic])\n",
    "        schema = avro.schema.parse(avro_schema_json)\n",
    "\n",
    "        for msg in consumer:\n",
    "            bytes_reader = io.BytesIO(msg.value)\n",
    "            decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "            reader = avro.io.DatumReader(schema)\n",
    "            user = reader.read(decoder)\n",
    "            print(user)\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class send_avro:    \n",
    "    @staticmethod\n",
    "    def send_avro_record_to_kafka(topic, record,avro_schema_json):\n",
    "        value_schema = avro.schema.parse(avro_schema_json)\n",
    "        writer = DatumWriter(value_schema)\n",
    "        bytes_writer = io.BytesIO()\n",
    "        encoder = BinaryEncoder(bytes_writer)\n",
    "        writer.write(record, encoder)\n",
    "        producer = KafkaProducer(bootstrap_servers=[KAFKA_BROKER])\n",
    "        producer.send(topic,value=bytes_writer.getvalue())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Kafka Intializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'35.196.152.2:9092'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KAFKA_BROKER=os.getenv('DEMO_KAFKA_BROKERS')\n",
    "KAFKA_BROKER=KAFKA_BROKER + \":9092\"\n",
    "KAFKA_BROKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_intializer=kafkatopic(KAFKA_BROKER,'Trip_Kafka')\n",
    "kafka_intializer.list_kafka_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Trip Data Kakfa Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Trip_Kafka created\n"
     ]
    }
   ],
   "source": [
    "kafka_intializer=kafkatopic(KAFKA_BROKER,'Trip_Kafka')\n",
    "kafka_intializer.create_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trip_Kafka': TopicMetadata(Trip_Kafka, 3 partitions)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin = AdminClient({'bootstrap.servers': KAFKA_BROKER})\n",
    "fs = admin.list_topics()\n",
    "fs.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema_json_trip = json.dumps({\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"TripStatistics\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"driver_id\", \"type\": \"long\"},\n",
    "        {\"name\": \"latitude_distance\", \"type\": \"double\"},\n",
    "        {\"name\": \"longitude_distance\", \"type\": \"double\"},\n",
    "         {\"name\": \"pickup_latitude\", \"type\": \"double\"},\n",
    "        {\"name\": \"dropoff_longitude\", \"type\": \"double\"},\n",
    "        {\"name\": \"pickup_longitude\", \"type\": \"double\"},\n",
    "        {\"name\": \"distance_travelled\", \"type\": \"double\"},\n",
    "        {\"name\": \"dropoff_latitude\", \"type\": \"double\"},\n",
    "\n",
    "        {\n",
    "            \"name\": \"pickup_datetime\",\n",
    "            \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_statistics.stream_source = KafkaSource(\n",
    "    event_timestamp_column=\"pickup_datetime\",\n",
    "    created_timestamp_column=\"datetime\",\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    topic=\"Trip_Kafka\",\n",
    "    message_format=AvroFormat(avro_schema_json_trip)\n",
    ")\n",
    "client.apply(trip_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.start_stream_to_online_ingestion(\n",
    "    trip_statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SparkJobStatus.IN_PROGRESS: 1>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_avro_record_to_kafka(topic, record,avro_schema_json):\n",
    "    value_schema = avro.schema.parse(avro_schema_json)\n",
    "    writer = DatumWriter(value_schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = BinaryEncoder(bytes_writer)\n",
    "    writer.write(record, encoder)\n",
    "    \n",
    "    producer = Producer({\n",
    "        \"bootstrap.servers\": KAFKA_BROKER,\n",
    "    })\n",
    "    producer.produce(topic=topic, value=bytes_writer.getvalue())\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in trip_ingest.iloc[:10000,:].drop(columns=['created']).to_dict('record'):\n",
    "    record[\"pickup_datetime\"] = (\n",
    "        record[\"pickup_datetime\"].to_pydatetime().replace(tzinfo=pytz.utc)\n",
    "    )\n",
    "    #send_avro.\n",
    "    send_avro.send_avro_record_to_kafka(\"Trip_Kafka\",record,avro_schema_json_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'driver_id': 610685},\n",
       " {'driver_id': 825735},\n",
       " {'driver_id': 428317},\n",
       " {'driver_id': 356886},\n",
       " {'driver_id': 603801},\n",
       " {'driver_id': 183971},\n",
       " {'driver_id': 600461},\n",
       " {'driver_id': 596197},\n",
       " {'driver_id': 382017},\n",
       " {'driver_id': 599864},\n",
       " {'driver_id': 486440},\n",
       " {'driver_id': 60412},\n",
       " {'driver_id': 318925},\n",
       " {'driver_id': 942474},\n",
       " {'driver_id': 111143},\n",
       " {'driver_id': 991464},\n",
       " {'driver_id': 239703},\n",
       " {'driver_id': 580070},\n",
       " {'driver_id': 640316},\n",
       " {'driver_id': 365485}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_sample = [{\"driver_id\": e} for e in trip_ingest.iloc[:100,:]['driver_id'].values.tolist()]\n",
    "entities_sample=entities_sample[:20]\n",
    "entities_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_statistics:pickup_longitude',\n",
       " 'trip_statistics:pickup_latitude',\n",
       " 'trip_statistics:dropoff_longitude',\n",
       " 'trip_statistics:dropoff_latitude',\n",
       " 'trip_statistics:longitude_distance',\n",
       " 'trip_statistics:latitude_distance',\n",
       " 'trip_statistics:distance_travelled']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = client.get_online_features(\n",
    "    feature_refs=driver_features,\n",
    "    entity_rows=entities_sample).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_statistics:dropoff_longitude</th>\n",
       "      <th>trip_statistics:dropoff_latitude</th>\n",
       "      <th>trip_statistics:distance_travelled</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>trip_statistics:pickup_latitude</th>\n",
       "      <th>trip_statistics:latitude_distance</th>\n",
       "      <th>trip_statistics:longitude_distance</th>\n",
       "      <th>trip_statistics:pickup_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>610685</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>-73.844311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>0.079696</td>\n",
       "      <td>825735</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>-74.016048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>428317</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>-73.982738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>356886</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>-73.987130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>603801</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>-73.968095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_statistics:dropoff_longitude  trip_statistics:dropoff_latitude  \\\n",
       "0                         -73.841610                         40.712278   \n",
       "1                         -73.979268                         40.782004   \n",
       "2                         -73.991242                         40.750562   \n",
       "3                         -73.991567                         40.758092   \n",
       "4                         -73.956655                         40.783762   \n",
       "\n",
       "   trip_statistics:distance_travelled  driver_id  \\\n",
       "0                            0.009436     610685   \n",
       "1                            0.079696     825735   \n",
       "2                            0.013674     428317   \n",
       "3                            0.025340     356886   \n",
       "4                            0.019470     603801   \n",
       "\n",
       "   trip_statistics:pickup_latitude  trip_statistics:latitude_distance  \\\n",
       "0                        40.721319                           0.009041   \n",
       "1                        40.711303                           0.070701   \n",
       "2                        40.761270                           0.010708   \n",
       "3                        40.733143                           0.024949   \n",
       "4                        40.768008                           0.015754   \n",
       "\n",
       "   trip_statistics:longitude_distance  trip_statistics:pickup_longitude  \n",
       "0                            0.002701                        -73.844311  \n",
       "1                            0.036780                        -74.016048  \n",
       "2                            0.008504                        -73.982738  \n",
       "3                            0.004437                        -73.987130  \n",
       "4                            0.011440                        -73.968095  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_kafka_data=pd.DataFrame(features)\n",
    "trip_kafka_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will stop the streaming job\n",
    "job.cancel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Fare Data kafka Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create topic Fare_Kafka: KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'Fare_Kafka' already exists.\"}\n"
     ]
    }
   ],
   "source": [
    "kafka_intializer=kafkatopic(KAFKA_BROKER,'Fare_Kafka')\n",
    "kafka_intializer.create_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trip_Kafka': TopicMetadata(Trip_Kafka, 3 partitions),\n",
       " 'Trip_Stats': TopicMetadata(Trip_Stats, 1 partitions),\n",
       " 'Fare_Kafka': TopicMetadata(Fare_Kafka, 3 partitions),\n",
       " '__consumer_offsets': TopicMetadata(__consumer_offsets, 50 partitions)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin = AdminClient({'bootstrap.servers': KAFKA_BROKER})\n",
    "fs = admin.list_topics()\n",
    "fs.topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema_json_fare = json.dumps({\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"FareStatistics\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"driver_id\", \"type\": \"long\"},\n",
    "        {\"name\": \"passenger_count\", \"type\": \"long\"},\n",
    "        {\"name\": \"target\", \"type\": \"long\"},\n",
    "         {\"name\": \"fare_amount\", \"type\": \"double\"},\n",
    "     \n",
    "        {\n",
    "            \"name\": \"pickup_datetime\",\n",
    "            \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    ],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_statistics.stream_source = KafkaSource(\n",
    "    event_timestamp_column=\"pickup_datetime\",\n",
    "    created_timestamp_column=\"datetime\",\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    topic=\"Fare_Kafka\",\n",
    "    message_format=AvroFormat(avro_schema_json_fare)\n",
    ")\n",
    "client.apply(fare_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.start_stream_to_online_ingestion(\n",
    "    fare_statistics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SparkJobStatus.IN_PROGRESS: 1>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in fare_details.iloc[:50,:].drop(columns=['created']).to_dict('record'):\n",
    "    record[\"pickup_datetime\"] = (\n",
    "        record[\"pickup_datetime\"].to_pydatetime().replace(tzinfo=pytz.utc)\n",
    "    )\n",
    "    #print(record)\n",
    "    send_avro_record_to_kafka(\"Fare_Kafka\",record, avro_schema_json_fare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'driver_id': 610685},\n",
       " {'driver_id': 825735},\n",
       " {'driver_id': 428317},\n",
       " {'driver_id': 356886},\n",
       " {'driver_id': 603801},\n",
       " {'driver_id': 183971},\n",
       " {'driver_id': 600461},\n",
       " {'driver_id': 596197},\n",
       " {'driver_id': 382017},\n",
       " {'driver_id': 599864}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_sample = [{\"driver_id\": e} for e in fare_details.iloc[:10,:]['driver_id'].values.tolist()]\n",
    "entities_sample=entities_sample[:20]\n",
    "entities_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = client.get_online_features(\n",
    "    feature_refs=fare_features,\n",
    "    entity_rows=entities_sample).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_statistics:target</th>\n",
       "      <th>fare_statistics:passenger_count</th>\n",
       "      <th>fare_statistics:fare_amount</th>\n",
       "      <th>driver_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.354113</td>\n",
       "      <td>610685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.088648</td>\n",
       "      <td>825735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.813646</td>\n",
       "      <td>428317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.191734</td>\n",
       "      <td>356886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.975267</td>\n",
       "      <td>603801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_statistics:target  fare_statistics:passenger_count  \\\n",
       "0                       0                                1   \n",
       "1                       1                                1   \n",
       "2                       0                                2   \n",
       "3                       0                                1   \n",
       "4                       0                                1   \n",
       "\n",
       "   fare_statistics:fare_amount  driver_id  \n",
       "0                    -1.354113     610685  \n",
       "1                     1.088648     825735  \n",
       "2                    -0.813646     428317  \n",
       "3                    -0.191734     356886  \n",
       "4                    -0.975267     603801  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_kafka_data=pd.DataFrame(features)\n",
    "fare_kafka_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will stop the streaming job\n",
    "job.cancel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Kubeflow Pipeline Artifacts for Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entity key**\n",
    "\n",
    "The combination of entities that uniquely identify a row. For example a feature table with the composite entity of \\(customer, country\\) might have an entity key of \\(1001, 5\\). They key is used during lookups of feature values and for deduplicating historical rows.\n",
    "\n",
    "**Entity timestamp**\n",
    "\n",
    "The timestamp on which an event occurred. The entity timestamp could describe the event time at which features were calculated, or it could describe the event timestamps at which outcomes were observed.\n",
    "\n",
    "Entity timestamps are commonly found on the entity dataframe and associated with the target variable \\(outcome\\) that needs to be predicted. These timestamps are the target on which point-in-time joins should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_parquet(uri):\n",
    "    parsed_uri = urlparse(uri)\n",
    "    if parsed_uri.scheme == \"file\":\n",
    "        return pd.read_parquet(parsed_uri.path)\n",
    "    elif parsed_uri.scheme == \"gs\":\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        files = [\"gs://\" + path for path in fs.glob(uri + '/part-*')]\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    elif parsed_uri.scheme == 's3':\n",
    "        import s3fs\n",
    "        fs = s3fs.S3FileSystem()\n",
    "        files = [\"s3://\" + path for path in fs.glob(uri + '/part-*')]\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    elif parsed_uri.scheme == 'wasbs':\n",
    "        import adlfs\n",
    "        fs = adlfs.AzureBlobFileSystem(\n",
    "            account_name=os.getenv('FEAST_AZURE_BLOB_ACCOUNT_NAME'), account_key=os.getenv('FEAST_AZURE_BLOB_ACCOUNT_ACCESS_KEY')\n",
    "        )\n",
    "        uripath = parsed_uri.username + parsed_uri.path\n",
    "        files = fs.glob(uripath + '/part-*')\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported URL scheme {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610685</td>\n",
       "      <td>2020-10-18 08:24:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>825735</td>\n",
       "      <td>2020-10-18 00:14:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>428317</td>\n",
       "      <td>2020-10-19 19:25:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356886</td>\n",
       "      <td>2020-10-19 21:55:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>603801</td>\n",
       "      <td>2020-10-19 13:31:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id     event_timestamp\n",
       "0     610685 2020-10-18 08:24:32\n",
       "1     825735 2020-10-18 00:14:50\n",
       "2     428317 2020-10-19 19:25:12\n",
       "3     356886 2020-10-19 21:55:33\n",
       "4     603801 2020-10-19 13:31:24"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_with_timestamp = pd.DataFrame(columns=['driver_id', 'event_timestamp'])\n",
    "entities_with_timestamp['driver_id'] = trip_details['driver_id']\n",
    "entities_with_timestamp['event_timestamp'] = pd.to_datetime(np.random.randint(\n",
    "    datetime(2020, 10, 18).timestamp(),\n",
    "    datetime(2020, 10, 20).timestamp(),\n",
    "    size=trip_details.shape[0]), unit='s')\n",
    "entities_with_timestamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_with_timestamp.to_csv('entity.csv',index=False)\n",
    "entities_with_timestamp.to_csv(\"gs://feastproject/driver_id.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fare_statistics:passenger_count',\n",
       " 'fare_statistics:fare_amount',\n",
       " 'fare_statistics:target',\n",
       " 'trip_statistics:pickup_longitude',\n",
       " 'trip_statistics:pickup_latitude',\n",
       " 'trip_statistics:dropoff_longitude',\n",
       " 'trip_statistics:dropoff_latitude',\n",
       " 'trip_statistics:longitude_distance',\n",
       " 'trip_statistics:latitude_distance',\n",
       " 'trip_statistics:distance_travelled']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Master_Features= fare_features + driver_features\n",
    "Master_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features.json\", \"w\") as output:\n",
    "    json.dump(Master_Features, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
